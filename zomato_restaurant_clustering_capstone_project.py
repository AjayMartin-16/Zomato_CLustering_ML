# -*- coding: utf-8 -*-
"""Zomato Restaurant CLustering Capstone Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193LjwMKxgrb6OL5KbnFproNzlJsh-Zs7

**Name : B. Ajay Martin Ferdinand**

**Project : Unsupervised ML - Zomato Restaurant Clustering**
"""

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/almabetter

import pandas as pd
df = pd.read_csv("Zomato Restaurant names and Metadata.csv")
df

df1 = pd.read_csv("Zomato Restaurant reviews.csv")
df1

df1.isna().sum()

df.isna().sum()

df.columns

df1.columns

"""# Data Pre-Processing"""

merged_df = pd.merge(df, df1, left_on='Name', right_on='Restaurant', how='left')

merged_df

"""Count Restaurants with No Reviews"""

# Count unmatched rows
no_reviews = merged_df[merged_df['Reviewer'].isna()]
print(f"Restaurants without reviews: {no_reviews.shape[0]} out of {df.shape[0]}")

# Fill missing values for review-related columns
merged_df.fillna({'Reviewer': 'No Reviewer', 'Review': 'No Review', 'Rating': 0}, inplace=True)

merged_df

"""Changing Data Types"""

merged_df['Cost'] = merged_df['Cost'].str.replace('₹', '').str.replace(',', '').astype(float)

"""**Comparission between missing reviews and having reviews**"""

# Extract rows with missing reviews
no_reviews_df = merged_df[merged_df['Reviewer'] == 'No Reviewer']

# Preview the no-review restaurants
print(no_reviews_df.head())

# Analyze cuisines for restaurants without reviews
no_review_cuisines = no_reviews_df['Cuisines'].value_counts()
print(no_review_cuisines)

# Average cost for restaurants without reviews
avg_cost_no_reviews = no_reviews_df['Cost'].mean()
print(f"Average Cost: {avg_cost_no_reviews}")

"""**Interpretations:**


1. **Cuisine Bias:**
Certain cuisines like American, Fast Food may have restaurants that are new or less reviewed compared to other popular cuisines such as North Indian or Chinese.
2. **Cost vs Reviews:**
If the average cost for restaurants with reviews is significantly different from ₹954.65, it could indicate that cost influences the likelihood of receiving reviews.
3. **Business Implication:**
Restaurants in the "no reviews" category may need to improve visibility or customer engagement strategies, such as promotions or targeted campaigns.


"""

import matplotlib.pyplot as plt

# Plot ratings
merged_df['Rating'].value_counts().plot(kind='bar', title='Ratings Distribution')
plt.xlabel('Ratings')
plt.ylabel('Count')
plt.show()

# Restaurants with reviews
with_reviews_df = merged_df[merged_df['Reviewer'] != 'No Reviewer']

# Average cost for restaurants with reviews
avg_cost_with_reviews = with_reviews_df['Cost'].mean()
print(f"Average Cost (with reviews): {avg_cost_with_reviews}")

# Compare cuisines
with_review_cuisines = with_reviews_df['Cuisines'].value_counts()
print(with_review_cuisines.head())

"""**Interpretations:**
1. Restaurants with reivews has less average price than the restaurants with no reviews.
2. This shows that due to some cost people are not wanted to give reviews.
3. Or maybe that Restaurants or not familiar with customers
4. Restaurants without reivews should promote their restaurants.

# Clustering Models
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download VADER lexicon
nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()

# Add sentiment scores
merged_df['Sentiment_Score'] = merged_df['Review'].apply(
    lambda x: sid.polarity_scores(str(x))['compound'] if pd.notna(x) else 0
)

# Aggregate metrics
sentiment_summary = merged_df.groupby('Name').agg({
    'Sentiment_Score': 'mean',  # Average sentiment score
    'Reviewer': 'count',        # Total number of reviews
    'Rating': lambda x: pd.to_numeric(x, errors='coerce').mean() # Average rating, handle non-numeric values
}).reset_index().rename(columns={'Reviewer': 'Total_Reviews'})

# Merge sentiment metrics back into metadata
cluster_data = pd.merge(df, sentiment_summary, on='Name', how='left')

# Fill missing values for restaurants without reviews
cluster_data['Sentiment_Score'] = cluster_data['Sentiment_Score'].fillna(0)  # Neutral sentiment
cluster_data['Total_Reviews'] = cluster_data['Total_Reviews'].fillna(0)     # No reviews
cluster_data['Rating'] = cluster_data['Rating'].fillna(cluster_data['Rating'].mean())  # Replace missing ratings with the mean

# Convert 'Cost' to numeric before filling NaN
cluster_data['Cost'] = pd.to_numeric(cluster_data['Cost'].str.replace('₹', '').str.replace(',', ''), errors='coerce')
# Now fill missing costs with the median
cluster_data['Cost'] = cluster_data['Cost'].fillna(cluster_data['Cost'].median())  # Replace missing costs with the median

# Features for clustering
features = cluster_data[['Cost', 'Sentiment_Score', 'Rating', 'Total_Reviews']]

# Standardize features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Calculate WCSS for different numbers of clusters
wcss = []
for k in range(2, 11):  # Test cluster sizes from 2 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')
plt.xticks(range(2, 11))
plt.grid()
plt.show()

from sklearn.cluster import KMeans
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.metrics import silhouette_score


for n_clusters in range(2,11):
    km = KMeans(n_clusters=n_clusters)
    preds = km.fit_predict(scaled_features)
    centers = km.cluster_centers_

    score = silhouette_score(scaled_features, preds, metric='euclidean')
    print('For n_clusters = {}, silhouette score is {}'.format(n_clusters, score))

    visualizer = SilhouetteVisualizer(km)

    visualizer.fit(scaled_features)
    visualizer.poof()

# Apply K-Means with the optimal number of clusters
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cluster_data['Cluster'] = kmeans.fit_predict(scaled_features)

# Scatter plot for Cost vs. Sentiment Score
plt.figure(figsize=(8, 6))
sns.scatterplot(data=cluster_data, x='Cost', y='Sentiment_Score', hue='Cluster', palette='viridis')
plt.title('Clusters by Cost and Sentiment Score')
plt.xlabel('Cost')
plt.ylabel('Sentiment Score')
plt.legend(title='Cluster')
plt.show()

"""**Clusters by Cost and Sentiment Score**
Interpretation:
* Restaurants with higher costs tend to have a higher sentiment score.
* Lower-cost restaurants show a mix of sentiment scores, with some having very low sentiment scores.
* The clusters (e.g., 0, 1, 2, 3) seem to segregate effectively based on cost and sentiment, reflecting varying levels of customer satisfaction or perceptions.

**Cluster Characteristics:**
* Cluster 0 likely represents restaurants with very low sentiment and possibly low cost.
* Cluster 3 might represent restaurants with moderate cost but higher sentiment scores.
* Cluster 1 includes restaurants with higher costs and consistently higher sentiment.
"""

# Scatter plot for Cost vs. Rating
plt.figure(figsize=(8, 6))
sns.scatterplot(data=cluster_data, x='Cost', y='Rating', hue='Cluster', palette='viridis')
plt.title('Clusters by Cost and Rating')
plt.xlabel('Cost')
plt.ylabel('Rating')
plt.legend(title='Cluster')
plt.show()

"""**Clusters by Cost and Rating**
Interpretation:
* Higher-cost restaurants tend to have higher ratings, but not exclusively.

* There are high-cost restaurants with moderate ratings.
* Lower-cost restaurants again show mixed ratings, indicating variability in quality perception.
* The clustering separates these variations well, showing distinct groupings based on cost and customer ratings.

**Cluster Characteristics:**
* Cluster 0 seems to include restaurants with very low ratings and low cost.
* Cluster 3 appears to capture mid-cost restaurants with strong ratings.
* Cluster 1 likely represents high-cost, high-rating restaurants.

**K-Means Model Interpretation:**

How this interpretation be useful for Restaurant Owners and Customers:

The clustering insights can be incredibly useful for various stakeholders, such as restaurant owners, customers, and marketing teams. Here's how these analyses can be applied:

**For Restaurant Owners**
1. Understanding Market Position:
 * Identify where their restaurant falls among competitors (e.g., high-cost, high-sentiment cluster vs. low-cost, low-rating cluster).
 * Understand their target customer demographic based on cost, sentiment, and ratings.

2. Improvement Opportunities:
 * Restaurants with low ratings can identify potential weaknesses like pricing mismatch or food quality issues.

3. Strategic Pricing:
 * Owners can align pricing strategies with clusters to compete better (e.g., lower cost to attract budget-conscious customers or raise it to appeal to premium-seekers).

**For Customers**
1. Restaurant Recommendations:
 * Customers seeking high-value restaurants can look at clusters with high ratings but moderate costs.
 * Customers looking for premium experiences can choose from high-cost, high-rating clusters.

**For Marketing Teams**
1. Targeted Advertising:
 * Design campaigns tailored to different clusters. For instance:
 * Premium restaurants (high-cost, high-rating clusters) could target luxury-seekers.
 * Budget restaurants (low-cost, moderate-rating clusters) could appeal to students or budget-conscious families.
2. Customized Promotions:
 * Offer discounts or promotions to clusters with low sentiment to attract more customers or improve perceptions.

**DBSCAN Algorithm**
"""

dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(scaled_features)

cluster_data['Cluster_DBSCAN'] = clusters  # Assign to cluster_data

# Step 3: Visualize the clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x='Cost',
    y='Sentiment_Score',
    hue='Cluster_DBSCAN',
    data=cluster_data,
    palette='tab10',
    style='Cluster_DBSCAN',
    markers={-1: 'X', 0: 'o', 1: 's', 2: 'D'},  # Define markers for each cluster including noise
)
plt.title('DBSCAN Clustering')
plt.xlabel('Cost')
plt.ylabel('Sentiment Score')
plt.legend(title='Cluster')
plt.show()

"""Plot 1: Sentiment Score vs. Cost
1. Axes:

 * X-axis: Cost (likely a monetary value for a product or service).
 * Y-axis: Sentiment Score (a measure of positive/negative sentiment, possibly derived from text analysis).

2. Clusters:

 * Cluster 0 (orange dots): Represents a group of data points with a cost range below 1500 and a wide range of positive sentiment scores (0.1 to 0.8).
 * Cluster 1 (green squares): Concentrated around a higher cost (1500 to 2000) with relatively high sentiment scores (0.5 to 0.8).
 * Noise (-1, blue crosses): Points identified as outliers. These are scattered across the plot, often at low or high costs and extreme sentiment scores (very low or high).
3. Observations:

 * Cluster 0 likely represents affordable products/services with varied sentiment.
 * Cluster 1 seems to represent higher-cost items with generally positive sentiment.
 * Noise points may be exceptional cases or anomalies, such as very expensive items with low sentiment scores.
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(
    x='Cost',
    y='Rating',
    hue='Cluster_DBSCAN',
    data=cluster_data,
    palette='tab10',
    style='Cluster_DBSCAN',
    markers={-1: 'X', 0: 'o', 1: 's', 2: 'D'},  # Define markers for each cluster including noise
)
plt.title('DBSCAN Clustering')
plt.xlabel('Cost')
plt.ylabel('Rating')
plt.legend(title='Cluster')
plt.show()

"""Plot 2: Rating vs. Cost
1. Axes:

 * X-axis: Cost (same as in Plot 1).
 * Y-axis: Rating (likely a numerical value, e.g., customer reviews, on a 5-point scale).
2. Clusters:

 * Cluster 0 (orange dots): Includes products/services with costs below ~1500 and ratings clustered around 3–4.
 * Cluster 1 (green squares): Includes higher-cost items (1500 to 2000) with ratings mostly above 4.
 * Noise (-1, blue crosses): These points have either very low ratings (close to 0) or are very high-cost items outside the main clusters.
3. Observations:

 *  Cluster 0 suggests that moderately priced items receive average ratings (3–4).
 * Cluster 1 indicates that high-cost items often receive high ratings (4–5).
 * Noise points may include products that are either poorly rated or exceptionally expensive.

**DBSCAN Interpretation**

### **For Restaurants**:  
- **Optimize Pricing**: Identify which dishes are worth pricing higher or need improvement.  
- **Promote Favorites**: Focus on high-cost, high-rating items to attract premium customers.  
- **Fix Weak Spots**: Address poorly rated dishes (noise points) or remove them.  

### **For Customers**:  
- **Find Value**: Choose affordable, high-rated dishes from Cluster 0.  
- **Pick Premium**: Go for high-rated, pricier options in Cluster 1 for quality experiences.  
- **Avoid Pitfalls**: Steer clear of outliers with low ratings and high costs.  

Both groups can use these insights to enhance their dining experience!
"""

